# Machine Learning Engineer Nanodegree
## Capstone Proposal
Lillian Hartmetz Neff
February 17, 2018

## Proposal

### Domain Background

Though many companies (such as Abelton, Google, Yousician) are working on problems such as music recommendations, song identification and classification, AI songwriting, and transposition. However, music information retrieval is under-researched when compared to natural language processing, image classification, etc. There have been relatively few attempts to solve the open problem of music transcription using machine learning. Digital transcription from audio to sheet music often involves first converting the audio files to MIDI. With this project, I propose that machine learning can be utilized to convert polyphonic audio files to MIDI. 

Related research:

The projects listed below all utilize machine learning to classify or generate music information.
- https://arxiv.org/pdf/1508.01774.pdf: An end-to-end neural network for polyphonic piano music transcription
- https://musicinformationretrieval.com: Instrument classification, evaluation, and genre recognition
- https://github.com/jsleep/wav2mid: Automatic music transcription with deep neural networks
- https://magenta.tensorflow.org: Music generation
- https://github.com/googlecreativelab/aiexperiments-ai-duet: Music generation
- https://deepjazz.io/: Music generation with an LSTM
- https://github.com/drscotthawley/audio-classifier-keras-cnn: Audio classification with CNN
- https://arxiv.org/pdf/1703.10847.pdf: A convolutional generative adversarial network for symbolic-domain music generation

Personal Motivation:

As a musician and songwriter, a tool that could take in audio and output sheet music would save hours of tedious manual labor. With this project, I hope to explore a potential new way to automate this process. There are some existing tools/companies that attempt this task such as HumOn, AnthemScore, Melodyne, and Ableton. I do not expect to solve automated transcription in this project. Instead, I hope this project will help prepare me for a job working on similar software or other audio signal processing work.

### Problem Statement

The problem to be solved here is the automated conversion of an audio file to a MIDI file. I propose a model that can take as input an audio file (mono- or polyphonic) and output a MIDI file with the corresponding notes and note duration. For the purposes of this project, not all information from the audio file will be explicitly retained. Some information loss is acceptable, such as dynamics (the volume of the notes being played) and the original instrument(s). 

I propose to create a neural network which will learn to identify notes as on or off, and the midi number (or pitch) of the notes which are present. 

The problem is quantifiable by comparing the original MIDI track with the MIDI track generated by the neural network, as a MIDI track consists of numeric values. As such, standard evaluation metrics can be applied and the problem is replicable. 

### Datasets and Inputs

In order to use a neural network and keep the output space small, I searched for a large audio/MIDI dataset with short, paired clips. Unfortunately, I did not find such a dataset, but I did find the Saarland Music Data. This dataset consists of 50 audio files and their temporally synchronized 50 MIDI files. I reduced the information in the MIDI file by first looking at the MIDI track in each file with the most messages, and using the most relevant information in each message. That information is whether a note is on or off, the pitch of each note turning on or off (represented as a MIDI value ranging from 0 to 127), and the MIDI time (delta ticks - the number of MIDI ticks since the last message). 

In order to increase my number of data points and have inputs small enough to feed into a neural network, I will segment the songs into short clips n seconds long. Additionally, for a neural network, the inputs must all have the same shape. Thus, if a song's length is not divisible by n, the song will be padded with silence at the end of the track. The same will be done for it's corresponding MIDI clip when the MIDI file is segmented into corresponding clips n seconds long.

There are several ways to represent audio in such a way that it can be fed into a neural network (or otherwise analyzed), such as a short-time Fourier transform and constant-Q transform. Both are time-frequency representations. I intend to use librosa's CQT function, which will ouput a numpy array with shape (n bins, time), the constant-Q value each frequency at each time. 

For the MIDI messages, the pitch value will be one-hot encoded. A pitch not being played will be represented with a 0. A pitch being played will be represented with a 1. The output size will be as follows: 2 * 128 possible pitches * number of possible discrete time values. 

Saarland Music Data:
https://www.audiolabs-erlangen.de/resources/MIR/SMD/midi
Meinard Müller, Verena Konz, Wolfgang Bogler, Vlora Arifi-Müller: Saarland Music Data (SMD). In Late-Breaking and Demo Session of the 12th International Conference on Music Information Retrieval (ISMIR), 2011.

### Solution Statement

Because of the way we can segment and shape audio data and because neural networks are well-suited for handling data of multiple dimensions, my first attempt will use a Convolutional Neural Network. CNNs are well-suited for problems for which the spatial information of the input (in this case, the presence of notes over time) needs to be preserved. In previous automatic music transcription research, such as An End-to-End Neural Network for Polyphonic Piano Music Transcription (referenced above), CNNs have proven to be relatively effective at determine pitches present within a given time frame.

I plan to construct a CNN of multiple layers. I intend to try multiple layer combinations. I expect my final network will have a max pooling layer after every one or two convolutional layers. I'll experiment with multiple activation functions, including ReLU, Sigmoid, softmax. I plan to train the NN on 75-80% of the data in order for it to predict the MIDI values for the testing set. The neural network’s output will be evaluated by the standard metrics for a regression problem (details below). This project will include the code to segment the Saarland Music Data as well as the segmented dataset itself.

### Benchmark Model

My benchmark for this will be chance: Given an audio file, the benchmark will return a random midi file from the dataset. The same evaluation metrics will be applied to the benchmark and the final model. 

### Evaluation Metrics

Given that this is a regression problem, mean absolute error, mean squared error, and R2 score will be used to quantify the performance of the benchmark and the neural network. These evaluation metrics will likely be utilized via a library such as keras. If not, they will be otherwise implemented in code.

### Project Design

The theoretical workflow for the rest of this project is as follows. I will first need to complete the audio processing, by which I mean I'll segment the audio and MIDI into clips based on seconds (this is 95% done) and pad any short samples from the end of a track with silent audio and silent MIDI. The function to remove extraneous information from the MIDI file is complete, so each MIDI files at this point is a (Python) list of messages. The function which will one-hot encode the MIDI output still needs to be written. The portion of code which will perform the constant-Q transform (or STFT) on the audio files is complete. Once this processing is complete, the code to establish the benchmark will be the next task.



-----------

**Before submitting your proposal, ask yourself. . .**

- Does the proposal you have written follow a well-organized structure similar to that of the project template?
- Is each section (particularly **Solution Statement** and **Project Design**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?
- Would the intended audience of your project be able to understand your proposal?
- Have you properly proofread your proposal to assure there are minimal grammatical and spelling mistakes?
- Are all the resources used for this project correctly cited and referenced?
