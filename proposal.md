# Machine Learning Engineer Nanodegree
## Capstone Proposal
Lillian Hartmetz Neff
February 17, 2018

## Proposal
_(approx. 2-3 pages)_

### Domain Background
_(approx. 1-2 paragraphs)_

Though many companies (Abelton, Google, Yousician) are working on problems such as music recommendations, song identification and classification, AI songwriting, and transposition. However, music information retrieval is a relatively under-researched field (compared to natural language processing, image classification, etc). There have been relatively few attempts to solve the open problem of music transcription using machine learning. Digital transcription from audio to sheet music often involves first converting the audio files to MIDI. With this project, I propose that machine learning can be utilized to convert polyphonic audio files to MIDI. 

Related research:

The projects listed below all utilize machine learning to classify or generate music information.
- https://arxiv.org/pdf/1508.01774.pdf: Polyphonic piano music transcription
- https://musicinformationretrieval.com: Instrument classification, evaluation, and genre recognition
- https://github.com/jsleep/wav2mid: Automatic music transcription with deep neural networks
- https://magenta.tensorflow.org: Music generation
- https://github.com/googlecreativelab/aiexperiments-ai-duet: Music generation
- https://deepjazz.io/: Music generation with an LSTM
- https://github.com/drscotthawley/audio-classifier-keras-cnn: Audio classification with CNN
- https://arxiv.org/pdf/1703.10847.pdf: A convolutional generative adversarial network for symbolic-domain music generation

Personal Motivation:
As a musician and songwriter, a tool that could take in audio and output sheet music would save hours of tedious manual labor. With this project, I hope to explore a potential new way to automate this process. There are some existing tools/companies that attempt this task such as HumOn, AnthemScore, Melodyne, and Ableton. Realistically, I do not expect to solve this problem alone. Therefore, I hope this project will help prepare me for a job working on this kind of software or other audio signal processing work.

### Problem Statement
_(approx. 1 paragraph)_

The problem to be solved here is the automated conversion of an audio file to MIDI. I propose a model that can take as input an audio file (mono- or polyphonic) and output a MIDI file which has the same notes with the same duration. For the purposes of this project, not all information from the audio file will be explicitly retained. Some information loss is acceptable, such as dynamics (amplitude), the original instrument, time signature, key, the octave of a note, and exact onsets. 

Taking an audio file as input, I propose to create a neural network (most likely CNN or RNN) which will learn to identify notes as on or off, and the midi number (or pitch) of the notes which are present. The problem is quantifiable by comparing the original MIDI track with the MIDI track generated by the neural network, as a MIDI track consists of numeric values. As such, standard evaluation metrics can be applied and the problem is replicable. 

### Datasets and Inputs
_(approx. 2-3 paragraphs)_

In order to use a neural network and keep the output space small, I searched for a large audio/MIDI dataset with short, paired clips. Unfortunately, I did not find such a dataset, but I did find the Saarland Music Data here: https://www.audiolabs-erlangen.de/resources/MIR/SMD/midi. It consists of 50 audio files and their temporally synchronized 50 MIDI files. I reduced the information in the MIDI file by first looking at the MIDI track in each file with the most messages, and using the most relevant information in each message. That information is whether a note is on or off, the pitch of each note turning on or off (represented as a MIDI value ranging from 0 to 127), and the MIDI time (delta ticks). 

I segmented the songs into lengths of four beats (as estimated by the librosa beat estimator). I generated a list of timestamps corresponding to every fourth beat and used that list to break up the corresponding MIDI files into their respective segments. This resulted in 8,963 audio files and their corresponding 8,963 MIDI files. For the MIDI messages, the pitch value will be one-hot encoded. 

Meinard Müller, Verena Konz, Wolfgang Bogler, Vlora Arifi-Müller: Saarland Music Data (SMD). In Late-Breaking and Demo Session of the 12th International Conference on Music Information Retrieval (ISMIR), 2011.

### Solution Statement
_(approx. 1 paragraph)_

For the purpose of keeping the output space small, note onsets and/or offsets will be at one of 12 ticks per beat. With four beats per MIDI file, the output space size is 48 ticks * 128 possible note values * 2 on/off. I will train a neural network on 75-80% of the data in order for it to predict these values for the testing set. The neural network’s output will be evaluated by the standard metrics for a regression problem. This project will include the code which segments the Saarland Music Data as well as the segmented dataset.

### Benchmark Model
_(approximately 1-2 paragraphs)_

My benchmark for this will be chance: given an audio will return a random midi file from the dataset.

an output which has a correct value for 50% of its values, and a random value (more likely to be a zero than a one) for the remaining 50%. The benchmark shape will be the same as the NN output. The same evaluation metrics will be applied. 

### Evaluation Metrics
_(approx. 1-2 paragraphs)_

Given that this is a regression problem, mean absolute error, mean squared error, and   will be used to quantify the performance of the benchmark and the neural network. 

 

These evaluation metrics will likely be utilized via a library such as keras. If not, they will be otherwise implemented in code.


### Project Design
_(approx. 1 page)_

Having the data prepared into four-beat segments, the next step is to encode the audio input to a format suitable to a neural network, making all audio inputs the same shape. This will likely involve a Fourier transform of some kind (standard, Short Term, or Fast). Additionally, I’ll one-hot encode the MIDI files. I plan to implement a Convolutional or Recurrent Neural Network. 

-----------

**Before submitting your proposal, ask yourself. . .**

- Does the proposal you have written follow a well-organized structure similar to that of the project template?
- Is each section (particularly **Solution Statement** and **Project Design**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?
- Would the intended audience of your project be able to understand your proposal?
- Have you properly proofread your proposal to assure there are minimal grammatical and spelling mistakes?
- Are all the resources used for this project correctly cited and referenced?
